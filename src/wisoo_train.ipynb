{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import horovod.tensorflow as hvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf2\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "from gpt2_keras.gpt2 import GPT2\n",
    "from gpt2_keras.builder import original_gpt2\n",
    "from gpt2_keras.builder.builder import build\n",
    "# from .builder.builder import build\n",
    "from gpt2_keras.encoder import get_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvd.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    if k == 0:\n",
    "        # no truncation\n",
    "        return logits\n",
    "\n",
    "    def _top_k():\n",
    "        values, _ = tf.nn.top_k(logits, k=k)\n",
    "        min_values = values[:, -1, tf.newaxis]\n",
    "        return tf.compat.v1.where(\n",
    "            logits < min_values,\n",
    "            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
    "            logits,\n",
    "        )\n",
    "    return tf.cond(\n",
    "        pred=tf.equal(k, 0),\n",
    "        true_fn=lambda: logits,\n",
    "        false_fn=lambda: _top_k(),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(\"./models/124M/hparams.json\") as f:\n",
    "    config = json.load(f)\n",
    "#\n",
    "gpt2 = GPT2(config, name='gpt2')\n",
    "\n",
    "# x= tf.placeholder(dtype=tf.int32, shape=[None, None])\n",
    "# y = gpt2(x)\n",
    "\n",
    "# print(type(config))\n",
    "\n",
    "# gpt2= build(config, \"./models/124M/model.ckpt.data-00000-of-00001\", name='gpt2')\n",
    "gpt2= build(config, \"./models/124M/model.ckpt\", name='gpt2')\n",
    "\n",
    "print(type(gpt2))\n",
    "# print(gpt2.layers[1].layers) # The Transformer\n",
    "\n",
    "embedding_layer = gpt2.layers[0]\n",
    "\n",
    "\n",
    "print(embedding_layer)  # The Embedding Layer\n",
    "\n",
    "\n",
    "print(\"printing vocab size:\",  embedding_layer.vocab_size) #50257\n",
    "print(\"printing word embedding:\",  embedding_layer.word_embedding) #(50257 , 768)=\n",
    "\n",
    "\n",
    "# gpt2.compile(\n",
    "#     optimizer=tf2.optimizers.RMSprop(lr=0.01),\n",
    "#     loss = tf2.keras.losses.MeanSquaredError(),\n",
    "#     metrics = ['accuracy']\n",
    "# )\n",
    "\n",
    "\n",
    "print(gpt2.summary())\n",
    "print(\"printing Transformer summary\")\n",
    "print(gpt2.layers[1].summary())\n",
    "\n",
    "# print(gpt2.summary())\n",
    "batch_size =1\n",
    "max_seq_length = 1024\n",
    "word_embedding = 768\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "# input1 = np.random.randint(embedding_layer.vocab_size, size=(batch_size, 5, embedding_layer.word_embedding[-1]))\n",
    "\n",
    "\n",
    "\n",
    "# input1 = np.random.randint(embedding_layer.vocab_size, size=(batch_size,max_seq_length))\n",
    "# output = gpt2(input1)\n",
    "# print(output)\n",
    "\n",
    "model_dir = \"./models/\"\n",
    "model_name = \"124M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc =get_encoder(model_name, model_dir)\n",
    "raw_text = \"What is interesting is the fact that the first\"\n",
    "raw_text1 = \"My family is doing fine.\"\n",
    "raw_text2 = \"But, I think\"\n",
    "# raw_text += '<|endoftext|>'\n",
    "bpe_tokens = enc.encode(raw_text)\n",
    "bpe_tokens1 = enc.encode(raw_text1)\n",
    "# bpe_tokens2 = enc.encode(raw_text2)\n",
    "\n",
    "\n",
    "\n",
    "print(\"bpe_tokens: \", bpe_tokens)\n",
    "print(\"bpe_tokens1: \", bpe_tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bpe_tokens1.append(50256)\n",
    "# print(bpe_tokens1)\n",
    "decoded = enc.decode(bpe_tokens)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(bpe_tokens1) != len(bpe_tokens):\n",
    "    bpe_tokens1.append(220)\n",
    "    \n",
    "print(\"bpe_tokens1 AFTER \\n pad: \", bpe_tokens1)\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "The tokens have to be either padded or be of the same length to be input as batch.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without the endoftext : [3792, 534, 1641, 880, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_token = enc.encoder['<|startoftext|>'] #\n",
    "end_token = enc.encoder['<|endoftext|>']\n",
    "\n",
    "# print(enc.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2 = gpt2([bpe_tokens])\n",
    "two_batch = [bpe_tokens, bpe_tokens1]\n",
    "output3 = gpt2(two_batch)\n",
    "\n",
    "print(\"**printing output2.shape**\")\n",
    "print(output2.shape)\n",
    "\n",
    "print(\"printing argmax of logits\")\n",
    "output2_int = np.argmax(output2, axis=2)\n",
    "\n",
    "\n",
    "print(\"**printing output3.shape**\")\n",
    "print(output3.shape)\n",
    "\n",
    "print(\"printing output3\")\n",
    "print(output3)\n",
    "\n",
    "\n",
    "\n",
    "print(output2_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer\n",
    "\n",
    "# tokenizer = GPT2Tokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_output = enc.decode(output2_int[0])\n",
    "print(decoded_output)\n",
    "\n",
    "\"\"\"\n",
    "The decoded output is similar to that of the RNN where one token gives one output.\n",
    "(This one output is the argmax of the gpt output, the immediate token that has the highest logit value)\n",
    "(So, top_k is basically taking k argmaxes from one row vector of the logit matrix)\n",
    "SO, for raw_text = \"What is interesting is the fact that the first\"\n",
    "\n",
    "the output is \"is the about that fact that the first time\"\n",
    "\n",
    "This means\n",
    "<input>      <output>\n",
    "What ==>     is\n",
    "is ===>      the\n",
    "interesting ===> about\n",
    "is ===> that\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_space = enc.encode(\" \")\n",
    "print(encoded_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.decode([220])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.decode([198])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.decode([12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"end_token is: \", start_token)\n",
    "print(\"end_token is: \", end_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"printing output2 :\", output2)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"printing output3 :\", output3)\n",
    "print(\"printing decoded output3\")\n",
    "output3_int_arr = np.argmax(output3, axis=2)\n",
    "decoded_output3 = [enc.decode(output3_int) for output3_int in output3_int_arr]\n",
    "print(decoded_output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(two_batch).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "For sparse_softmax_cross_entropy_with_logits, labels must have the shape [batch_size] and the dtype int32 or int64. Each label is an int in range [0, num_classes-1].\n",
    "For softmax_cross_entropy_with_logits, labels must have the shape [batch_size, num_classes] and dtype float32 or float64.\n",
    "Labels used in softmax_cross_entropy_with_logits are the one hot version of labels used in sparse_softmax_cross_entropy_with_logits.\n",
    "\n",
    "Another tiny difference is that with sparse_softmax_cross_entropy_with_logits, you can give -1 as a label to have loss 0 on this label.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(two_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=np.array(two_batch)[:, 1:],\n",
    "            logits=output3[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(\n",
    "        input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=np.array(two_batch)[:, 1:],\n",
    "            logits=output3[:, :-1])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Provided optimizer doesn't inherit from either legacy TensorFlow or Keras optimizer: <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-98cdbc7c1f58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/cs109b/lib/python3.7/site-packages/horovod/tensorflow/__init__.py\u001b[0m in \u001b[0;36mDistributedOptimizer\u001b[0;34m(optimizer, name, use_locking, device_dense, device_sparse, compression, sparse_as_dense, backward_passes_per_step, op)\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         raise ValueError('Provided optimizer doesn\\'t inherit from either legacy '\n\u001b[0;32m--> 471\u001b[0;31m                          'TensorFlow or Keras optimizer: %s' % optimizer)\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Provided optimizer doesn't inherit from either legacy TensorFlow or Keras optimizer: <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>"
     ]
    }
   ],
   "source": [
    "optimizer = tf2.keras.optimizers.Adam\n",
    "optimizer = hvd.DistributedOptimizer(optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs109b",
   "language": "python",
   "name": "cs109b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
