{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'horovod'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-e90ed6cabab4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhorovod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhvd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'horovod'"
     ]
    }
   ],
   "source": [
    "import horovod.tensorflow as hvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/share/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Restoring parameters from ./models/124M/model.ckpt\n",
      "<class 'gpt2_keras.builder.gpt2.GPT2'>\n",
      "<gpt2_keras.builder.gpt2.Embedding object at 0x7fcbd13ada90>\n",
      "printing vocab size: 50257\n",
      "printing word embedding: <tf.Variable 'gpt2/embedding/word_embedding:0' shape=(50257, 768) dtype=float32, numpy=\n",
      "array([[-0.11010301, -0.03926672,  0.03310751, ..., -0.1363697 ,\n",
      "         0.01506208,  0.04531523],\n",
      "       [ 0.04034033, -0.04861503,  0.04624869, ...,  0.08605453,\n",
      "         0.00253983,  0.04318958],\n",
      "       [-0.12746179,  0.04793796,  0.18410145, ...,  0.08991534,\n",
      "        -0.12972379, -0.08785918],\n",
      "       ...,\n",
      "       [-0.04453601, -0.05483596,  0.01225674, ...,  0.10435229,\n",
      "         0.09783269, -0.06952604],\n",
      "       [ 0.1860082 ,  0.01665728,  0.04611587, ..., -0.09625227,\n",
      "         0.07847701, -0.02245961],\n",
      "       [ 0.05135201, -0.02768905,  0.0499369 , ...,  0.00704835,\n",
      "         0.15519823,  0.12067825]], dtype=float32)>\n",
      "Model: \"gpt2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  39383808  \n",
      "_________________________________________________________________\n",
      "transformer (Transformer)    multiple                  85056000  \n",
      "=================================================================\n",
      "Total params: 124,439,808\n",
      "Trainable params: 124,439,808\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "printing Transformer summary\n",
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block_0 (Block)              multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "block_1 (Block)              multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "block_2 (Block)              multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "block_3 (Block)              multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "block_4 (Block)              multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "block_5 (Block)              multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "block_6 (Block)              multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "block_7 (Block)              multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "block_8 (Block)              multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "block_9 (Block)              multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "block_10 (Block)             multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "block_11 (Block)             multiple                  7087872   \n",
      "_________________________________________________________________\n",
      "layer_norm (LayerNormalizati multiple                  1536      \n",
      "=================================================================\n",
      "Total params: 85,056,000\n",
      "Trainable params: 85,056,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf2\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "from gpt2_keras.gpt2 import GPT2\n",
    "from gpt2_keras.builder import original_gpt2\n",
    "from gpt2_keras.builder.builder import build\n",
    "# from .builder.builder import build\n",
    "from gpt2_keras.encoder import get_encoder\n",
    "\n",
    "\n",
    "\n",
    "def top_k_logits(logits, k):\n",
    "    if k == 0:\n",
    "        # no truncation\n",
    "        return logits\n",
    "\n",
    "    def _top_k():\n",
    "        values, _ = tf.nn.top_k(logits, k=k)\n",
    "        min_values = values[:, -1, tf.newaxis]\n",
    "        return tf.compat.v1.where(\n",
    "            logits < min_values,\n",
    "            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
    "            logits,\n",
    "        )\n",
    "    return tf.cond(\n",
    "        pred=tf.equal(k, 0),\n",
    "        true_fn=lambda: logits,\n",
    "        false_fn=lambda: _top_k(),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(\"./models/124M/hparams.json\") as f:\n",
    "    config = json.load(f)\n",
    "#\n",
    "gpt2 = GPT2(config, name='gpt2')\n",
    "\n",
    "# x= tf.placeholder(dtype=tf.int32, shape=[None, None])\n",
    "# y = gpt2(x)\n",
    "\n",
    "# print(type(config))\n",
    "\n",
    "# gpt2= build(config, \"./models/124M/model.ckpt.data-00000-of-00001\", name='gpt2')\n",
    "gpt2= build(config, \"./models/124M/model.ckpt\", name='gpt2')\n",
    "\n",
    "print(type(gpt2))\n",
    "# print(gpt2.layers[1].layers) # The Transformer\n",
    "\n",
    "embedding_layer = gpt2.layers[0]\n",
    "\n",
    "\n",
    "print(embedding_layer)  # The Embedding Layer\n",
    "\n",
    "\n",
    "print(\"printing vocab size:\",  embedding_layer.vocab_size) #50257\n",
    "print(\"printing word embedding:\",  embedding_layer.word_embedding) #(50257 , 768)=\n",
    "\n",
    "\n",
    "# gpt2.compile(\n",
    "#     optimizer=tf2.optimizers.RMSprop(lr=0.01),\n",
    "#     loss = tf2.keras.losses.MeanSquaredError(),\n",
    "#     metrics = ['accuracy']\n",
    "# )\n",
    "\n",
    "\n",
    "print(gpt2.summary())\n",
    "print(\"printing Transformer summary\")\n",
    "print(gpt2.layers[1].summary())\n",
    "\n",
    "# print(gpt2.summary())\n",
    "batch_size =1\n",
    "max_seq_length = 1024\n",
    "word_embedding = 768\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "# input1 = np.random.randint(embedding_layer.vocab_size, size=(batch_size, 5, embedding_layer.word_embedding[-1]))\n",
    "\n",
    "\n",
    "\n",
    "# input1 = np.random.randint(embedding_layer.vocab_size, size=(batch_size,max_seq_length))\n",
    "# output = gpt2(input1)\n",
    "# print(output)\n",
    "\n",
    "model_dir = \"./models/\"\n",
    "model_name = \"124M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpe_tokens:  [2061, 318, 3499, 318, 262, 1109, 326, 262, 717]\n",
      "bpe_tokens1:  [3666, 1641, 318, 1804, 3734, 13]\n"
     ]
    }
   ],
   "source": [
    "enc =get_encoder(model_name, model_dir)\n",
    "raw_text = \"What is interesting is the fact that the first\"\n",
    "raw_text1 = \"My family is doing fine.\"\n",
    "raw_text2 = \"But, I think\"\n",
    "# raw_text += '<|endoftext|>'\n",
    "bpe_tokens = enc.encode(raw_text)\n",
    "bpe_tokens1 = enc.encode(raw_text1)\n",
    "# bpe_tokens2 = enc.encode(raw_text2)\n",
    "\n",
    "\n",
    "\n",
    "print(\"bpe_tokens: \", bpe_tokens)\n",
    "print(\"bpe_tokens1: \", bpe_tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is interesting is the fact that the first\n"
     ]
    }
   ],
   "source": [
    "# bpe_tokens1.append(50256)\n",
    "# print(bpe_tokens1)\n",
    "decoded = enc.decode(bpe_tokens)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpe_tokens1 AFTER \n",
      " pad:  [3666, 1641, 318, 1804, 3734, 13, 220, 220, 220]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe tokens have to be either padded or be of the same length to be input as batch.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "while len(bpe_tokens1) != len(bpe_tokens):\n",
    "    bpe_tokens1.append(220)\n",
    "    \n",
    "print(\"bpe_tokens1 AFTER \\n pad: \", bpe_tokens1)\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "The tokens have to be either padded or be of the same length to be input as batch.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without the endoftext : [3792, 534, 1641, 880, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_token = enc.encoder['<|startoftext|>'] #\n",
    "end_token = enc.encoder['<|endoftext|>']\n",
    "\n",
    "# print(enc.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**printing output2.shape**\n",
      "(1, 9, 50257)\n",
      "printing argmax of logits\n",
      "**printing output3.shape**\n",
      "(2, 9, 50257)\n",
      "printing output3\n",
      "tf.Tensor(\n",
      "[[[ -37.217266  -36.886486  -40.356358 ...  -43.435127  -43.023254\n",
      "    -37.09941 ]\n",
      "  [ -84.74174   -82.64531   -88.28572  ...  -89.45007   -90.05545\n",
      "    -84.677   ]\n",
      "  [ -77.438644  -74.91486   -83.047714 ...  -85.73609   -89.438644\n",
      "    -77.72409 ]\n",
      "  ...\n",
      "  [-103.48562  -103.50233  -107.460785 ... -105.617294 -109.159485\n",
      "   -104.502495]\n",
      "  [ -93.457886  -92.13289   -94.69766  ...  -90.95544   -96.78728\n",
      "    -92.80166 ]\n",
      "  [ -99.954926  -99.49396  -104.49782  ... -102.37962  -104.86124\n",
      "   -101.482704]]\n",
      "\n",
      " [[ -33.07355   -32.334896  -35.237953 ...  -38.357666  -38.475765\n",
      "    -33.09432 ]\n",
      "  [-113.07895  -111.44274  -120.889984 ... -120.88999  -119.589806\n",
      "   -115.100845]\n",
      "  [-120.354065 -119.78439  -124.40173  ... -125.55103  -126.36237\n",
      "   -122.76428 ]\n",
      "  ...\n",
      "  [ -73.21464   -69.45617   -74.36198  ...  -83.406815  -82.485275\n",
      "    -72.76322 ]\n",
      "  [ -32.82277   -29.06462   -33.214638 ...  -42.720463  -42.29901\n",
      "    -30.226212]\n",
      "  [ -36.339565  -33.12221   -36.734196 ...  -45.855415  -45.817616\n",
      "    -32.856327]]], shape=(2, 9, 50257), dtype=float32)\n",
      "[[ 318  262  546  326 1109  326  262  717  640]]\n"
     ]
    }
   ],
   "source": [
    "output2 = gpt2([bpe_tokens])\n",
    "two_batch = [bpe_tokens, bpe_tokens1]\n",
    "output3 = gpt2(two_batch)\n",
    "\n",
    "print(\"**printing output2.shape**\")\n",
    "print(output2.shape)\n",
    "\n",
    "print(\"printing argmax of logits\")\n",
    "output2_int = np.argmax(output2, axis=2)\n",
    "\n",
    "\n",
    "print(\"**printing output3.shape**\")\n",
    "print(output3.shape)\n",
    "\n",
    "print(\"printing output3\")\n",
    "print(output3)\n",
    "\n",
    "\n",
    "\n",
    "print(output2_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer\n",
    "\n",
    "# tokenizer = GPT2Tokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is the about that fact that the first time\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe decoded output is similar to that of the RNN where one token gives one output.\\n(This one output is the argmax of the gpt output, the immediate token that has the highest logit value)\\n(So, top_k is basically taking k argmaxes from one row vector of the logit matrix)\\nSO, for raw_text = \"What is interesting is the fact that the first\"\\n\\nthe output is \"is the about that fact that the first time\"\\n\\nThis means\\n<input>      <output>\\nWhat ==>     is\\nis ===>      the\\ninteresting ===> about\\nis ===> that\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_output = enc.decode(output2_int[0])\n",
    "print(decoded_output)\n",
    "\n",
    "\"\"\"\n",
    "The decoded output is similar to that of the RNN where one token gives one output.\n",
    "(This one output is the argmax of the gpt output, the immediate token that has the highest logit value)\n",
    "(So, top_k is basically taking k argmaxes from one row vector of the logit matrix)\n",
    "SO, for raw_text = \"What is interesting is the fact that the first\"\n",
    "\n",
    "the output is \"is the about that fact that the first time\"\n",
    "\n",
    "This means\n",
    "<input>      <output>\n",
    "What ==>     is\n",
    "is ===>      the\n",
    "interesting ===> about\n",
    "is ===> that\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220]\n"
     ]
    }
   ],
   "source": [
    "encoded_space = enc.encode(\" \")\n",
    "print(encoded_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode([220])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode([198])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode([12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end_token is:  50256\n"
     ]
    }
   ],
   "source": [
    "# print(\"end_token is: \", start_token)\n",
    "print(\"end_token is: \", end_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing output2 : tf.Tensor(\n",
      "[[[ -37.217266  -36.886486  -40.356358 ...  -43.435127  -43.023254\n",
      "    -37.09941 ]\n",
      "  [ -84.74174   -82.64531   -88.28572  ...  -89.45007   -90.05545\n",
      "    -84.677   ]\n",
      "  [ -77.438644  -74.91486   -83.047714 ...  -85.73609   -89.438644\n",
      "    -77.72409 ]\n",
      "  ...\n",
      "  [-103.48562  -103.50233  -107.460785 ... -105.617294 -109.159485\n",
      "   -104.502495]\n",
      "  [ -93.457886  -92.13289   -94.69766  ...  -90.95544   -96.78728\n",
      "    -92.80166 ]\n",
      "  [ -99.95494   -99.49397  -104.49782  ... -102.37963  -104.86125\n",
      "   -101.482704]]], shape=(1, 9, 50257), dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"printing output2 :\", output2)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing output3 : tf.Tensor(\n",
      "[[[ -37.217266  -36.886486  -40.356358 ...  -43.435127  -43.023254\n",
      "    -37.09941 ]\n",
      "  [ -84.74174   -82.64531   -88.28572  ...  -89.45007   -90.05545\n",
      "    -84.677   ]\n",
      "  [ -77.438644  -74.91486   -83.047714 ...  -85.73609   -89.438644\n",
      "    -77.72409 ]\n",
      "  ...\n",
      "  [-103.48562  -103.50233  -107.460785 ... -105.617294 -109.159485\n",
      "   -104.502495]\n",
      "  [ -93.457886  -92.13289   -94.69766  ...  -90.95544   -96.78728\n",
      "    -92.80166 ]\n",
      "  [ -99.954926  -99.49396  -104.49782  ... -102.37962  -104.86124\n",
      "   -101.482704]]\n",
      "\n",
      " [[ -33.07355   -32.334896  -35.237953 ...  -38.357666  -38.475765\n",
      "    -33.09432 ]\n",
      "  [-113.07895  -111.44274  -120.889984 ... -120.88999  -119.589806\n",
      "   -115.100845]\n",
      "  [-120.354065 -119.78439  -124.40173  ... -125.55103  -126.36237\n",
      "   -122.76428 ]\n",
      "  ...\n",
      "  [ -73.21464   -69.45617   -74.36198  ...  -83.406815  -82.485275\n",
      "    -72.76322 ]\n",
      "  [ -32.82277   -29.06462   -33.214638 ...  -42.720463  -42.29901\n",
      "    -30.226212]\n",
      "  [ -36.339565  -33.12221   -36.734196 ...  -45.855415  -45.817616\n",
      "    -32.856327]]], shape=(2, 9, 50257), dtype=float32)\n",
      "printing decoded output3\n",
      "[' is the about that fact that the first time', '\\n and very well. We\\xa0\\xa0 ']\n"
     ]
    }
   ],
   "source": [
    "print(\"printing output3 :\", output3)\n",
    "print(\"printing decoded output3\")\n",
    "output3_int_arr = np.argmax(output3, axis=2)\n",
    "decoded_output3 = [enc.decode(output3_int) for output3_int in output3_int_arr]\n",
    "print(decoded_output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 9)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(two_batch).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nFor sparse_softmax_cross_entropy_with_logits, labels must have the shape [batch_size] and the dtype int32 or int64. Each label is an int in range [0, num_classes-1].\\nFor softmax_cross_entropy_with_logits, labels must have the shape [batch_size, num_classes] and dtype float32 or float64.\\nLabels used in softmax_cross_entropy_with_logits are the one hot version of labels used in sparse_softmax_cross_entropy_with_logits.\\n\\nAnother tiny difference is that with sparse_softmax_cross_entropy_with_logits, you can give -1 as a label to have loss 0 on this label.\\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "For sparse_softmax_cross_entropy_with_logits, labels must have the shape [batch_size] and the dtype int32 or int64. Each label is an int in range [0, num_classes-1].\n",
    "For softmax_cross_entropy_with_logits, labels must have the shape [batch_size, num_classes] and dtype float32 or float64.\n",
    "Labels used in softmax_cross_entropy_with_logits are the one hot version of labels used in sparse_softmax_cross_entropy_with_logits.\n",
    "\n",
    "Another tiny difference is that with sparse_softmax_cross_entropy_with_logits, you can give -1 as a label to have loss 0 on this label.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2061, 318, 3499, 318, 262, 1109, 326, 262, 717], [3666, 1641, 318, 1804, 3734, 13, 220, 220, 220]]\n"
     ]
    }
   ],
   "source": [
    "print(two_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=15971, shape=(2, 8), dtype=float32, numpy=\n",
       "array([[3.1975884 , 6.500888  , 1.1376996 , 2.762639  , 1.7081244 ,\n",
       "        0.04474313, 1.8809332 , 4.913712  ],\n",
       "       [6.4828143 , 1.9557304 , 5.2614756 , 2.9690232 , 1.339585  ,\n",
       "        8.436424  , 6.941495  , 1.6103457 ]], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=np.array(two_batch)[:, 1:],\n",
    "            logits=output3[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(\n",
    "        input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=np.array(two_batch)[:, 1:],\n",
    "            logits=output3[:, :-1])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.5714514, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
