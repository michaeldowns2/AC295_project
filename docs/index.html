<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" href="../../favicon.ico">

    <title>CS205 Final Project</title>

    <!-- NOTE: USING BOOTSTRAP 3.4 -->
    
    <!-- Bootstrap core CSS -->
    <link href="./css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="./css/ie10-viewport-bug-workaround.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="./css/theme.css" rel="stylesheet">

    <!-- IACS stylesheet -->
    <link rel="stylesheet" href="./style/css/iacs.css" />


    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>
    <!-- Fixed navbar -->
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#top">CS205 Project</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="#top">Home</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container theme-showcase" role="main">

      <!-- Main jumbotron for a primary marketing message or call to action -->
      <div class="jumbotron">
        <h1>Textual Style Transfer with GPT-2</h1>
        <p><b>Group 8</b> - Michael <u>Downs</u>, Cameron <u>Hickert</u>, Wen Rui <u>Liau</u>, Wisoo <u>Song</u></p>
      </div>

      <div class="page-header">
        <h1>Overview</h1>
      </div>
        <ol class="list-group">
        <a href="#1" class="list-group-item list-group-item-info">Description of problem and the need for HPC and/or Big Data</a>
        <a href="#2" class="list-group-item list-group-item-success">Description of solution and comparison with existing work on the problem</a>
        <a href="#3" class="list-group-item list-group-item-info">Description of your model and/or data in detail</a>
        <a href="#4" class="list-group-item list-group-item-success">Technical description of the parallel application, programming models, platform and infrastructure</a>
        <a href="#5" class="list-group-item list-group-item-info">Links to repository with source code, evaluation data sets and test cases</a>
        <a href="#6" class="list-group-item list-group-item-success">Technical description of the software design, code baseline, dependencies, how to use the code, and system and environment needed to reproduce your tests</a>
        <a href="#7" class="list-group-item list-group-item-info">Performance evaluation (speed-up, throughput, weak and strong scaling) and discussion about overheads and optimizations done</a>
        <a href="#8" class="list-group-item list-group-item-success">Description of advanced features</a>
        <a href="#9" class="list-group-item list-group-item-info">Final discussion about goals achieved, improvements suggested, lessons learnt, future work, interesting insights</a>
        <a href="#10" class="list-group-item list-group-item-success">Citations</a>
      </ol>

      <a class="anchor" id="1"></a>
      <div class="page-header">
        <h2>1. Description of problem and the need for HPC and/or Big Data</h2>
      </div>
      <p> In our project, we will be exploring the problem of Textual Style Transfer with GPT-2. Two facets of any written work are style and content. <em>Content</em> pertains to the events or ideas being described whilst <em>Style</em> pertains to the manner in which the events or ideas are being described. Different authors and/or mediums have different styles. Textual style transfer aims to modify the style of one work to mimic the style of another while keeping the content relevant to the original. We will be using the GPT-2 model developed by OpenAI to perform textual style transfer on diverse and large datasets. </p>

      <p> There is a need for both big compute and big data processing in our project. On the <b>Big Compute</b> front, we will need a lot of compute resources to train the GPT-2 model for textural style transfer. GPT-2 comes in various sizes and all have a significant number of parameters to train. The "small" model that we are working with has 117 million parameters to train and our rough time estimate shows that we require 6 hours on single GPU to get reasonable (conditional) initial results using 7MB of text and 33k iterations. HPC will allow us to speed this up. </p>

      <p>On the <b>Big Data</b> front, the original GPT-2 model is originally trained with 40GB of internet text. This includes WikiText, which contains 100 million tokens from  verified “good” and “featured” Wikipedia articles as well as Treebank-2 which contains 1 million tokens from WSJ articles. The large sizes of the datasets that we are dealing with implies a clear for Big Data processing for us to handle these cases.</p> 

      <a class="anchor" id="2"></a>
      <div class="page-header">
        <h2>2. Description of solution and comparison with existing work on the problem</h2>
      </div>
      <p> In scoping the existing work on this problem, we refer to the paper and associated codebase written by OpenAI linked <a href = https://openai.com/blog/better-language-models/>here</a>. OpenAI provides the GPT-2 model architecture that they tested for Textual Style Transfer. However, they do not provide the trained parameters for this problem. As such, our solution involves training this model to do textual style transfer. Moreover in comparison with the original solution, our solution will also train the parameters in parallel which we will further elaborate below. </p>

      <a class="anchor" id="3"></a>
      <div class="page-header">
        <h2>3. Description of your model and/or data in detail</h2>
      </div>
      <p> I will first describe the source data that we used for our textual style transfer application. For our source textual data, we used the zothique stories obtained from <a href = khttp://www.eldritchdark.com/> this link </a> and project gutenberg books obtained using from this <a href = https://github.com/pgcorpus/gutenberg/tree/master/src> repository </a>. The raw project gutenberg corpus is almost 9GB compressed that requires processing. We used the super_cleaner functionality from this <a href = https://github.com/kiasar/gutenberg_cleaner> repository </a> to strip of all of the project gutenberg boilerplate. The pgcorpus repository also generates a metadata file while we used to separate the project gutenberg books into the genres: juvenile, 19th century, science fiction, adventure, fairy tales, and poetry. Of those, we only used the first three.</p>

      <p> The core model we used for textual style transfer is GPT-2. GPT-2 is a transformer-based Neural Net for unsupervised Multitask Learning. This model consists of an Encoder as well as a Decoder and the model architecture is seen below. In the original paper by OpenAI, this model is able to perform zero-shot reading comprehension, summarization, translation and question answering to varying degrees of success.</p>

      <img src="./img/gpt2.png" class="center-block" width=400px>


      <p> In describing our model for textual style transfer, I will first outline the core algorithm in generating new text from our corpus of data outlined above. The algorithm for generating text takes as input a pre-trained GPT-2 and a text file. The text file is chunked into contiguous groups of sentences which are used as prompts for GPT-2. The contiguous group is then shifted by some step size. For this project, the chunk/window size was <b>9</b> and the step size was <b>4</b>. 4 sentences were generated at a time using GPT-2. We used nltk as the sentence tokenizer. </p>

      <a class="anchor" id="4"></a>
      <div class="page-header">
        <h2>4. Technical description of the parallel application, programming models, platform and infrastructure</h2>
      </div>
      <p> Our parallel application involves exploiting a range of different parallelism levels including many-node and many-core parallelism as we used multiple GPUs on multiple nodes. There are two main areas to parallelize: the training phase and the generation phase. </p>
      <p> In the training phase of our project, the transformer computations and gradient updates in GPT-2 fine-tuning will be parallelized. In the generation phase, we will be parallelizing the generation of style-transfered text from a source file. </p>
      <p> In deciding on the parallel programming model to use, we used the ring all-reduce algorithm via Horovod that utilizes open MPI. Compared to the alternative of distributed tensorflow's worker-parameter server model, Horovod's ring all-reduce algorithm is superior in terms of both speed as well as space due to the absence of the need for a parameter server. An overview of this parallel algorithm is shown below. </p>

      <img src="./img/ringallreduce.png" class="center-block" width=1000px>

      <p> Our language of choice will be Python which allows us to access a wide range of different libraries for our project. Our machine learning framework used is Tensorflow and Keras as well as Horovod as mentioned above. Our platform operating system used is Deep Learning Amazon Machine Image (Ubuntu) which came with Tensorflow and Horovod pre-configured. Our infrastructure of choice is Amazon's Elastic Compute 2 as well as Harvard's Cannon supercomputer.Specifically on AWS's EC2 cluster, we used a g4dn.12xlarge instance which contains 4 Nvidia T4 Tensor Core GPUs, 48 vCPUs, and 192Gb of RAM.</p>

      <a class="anchor" id="5"></a>
      <div class="page-header">
        <h2>5. Links to repository with source code, evaluation data sets and test cases</h2>
      </div>
      <p> Our entire codebase for this project is hosted on Wisoo's github page linked <a href=https://github.com/osiajod/cs205_project>here</a>. In that page, we have also linked a folder titled <u>results</u> which contain our outputs from training our GPT-2 model on a variety of different nodes on AWS. These outputs allow us to better understand the accuracy and speed of scaling of GPT-2 which we will present in the plots below. We have also included several style-transfered texts that our generated from our trained GPT-2 model located under the <u>style_transferred</u> folder. The target texts for style transfer are a wikipedia article on MC Escher, a news article on hurricanes in the gulf of mexico, and a short story by jorge luis borges. The filenames follow the following naming convention <u>style_numepochs_target_window_step</u>. For example: 19th_1200_escher_9_4. These style transfered texts are how we qualitatively evaluated the effectiveness of our model in performing the textual style transfer. As an example, here is an excerpt taken from <u>juv_900_hurricance_9_4</u>. </p>
      <blockquote class="blockquote">
      <p class="mb-0">
      May is the country in which most of these men in America travel in the late-season and in a level sense of the country, so strong an effort to move across to reinforce new shipping during the autumn will appear with gaining strength at extreme hours when both men are already together. Thus far only, that time before the 1915 sick-leave of the African coast of Mexico has been fostering with the passage between San Diego and the Rio Guayana Warinar region.
      </p>
      <footer class="blockquote-footer">Execerpt from juv_900_hurricance_9_4</footer>
      </blockquote>

      <p> During our training phase, we will be timing our training of GPT-2 on two different sources of text from Eldritchdark and Project Gutenberg. We will be using the Zothique text as well as the Juvenile text. The Zothique text represents the smallest source of text (201Kb zipped) avaliable on Project Gutenberg whilst the Juvenile text represents the largest source of text (484Mb zipped). Using this spectrum of sizes allows us to better appreciate the performance of our model outlined in <a href="#7">Section 7</a> below. </p>

      <a class="anchor" id="6"></a>
      <div class="page-header">
        <h2>6. Technical description of the software design, code baseline, dependencies, how to use the code, and system and environment needed to reproduce your tests</h2>
      </div>
      <p> <b>Will probably need to fill in our instructions on running.</b></p>

      <a class="anchor" id="7"></a>
      <div class="page-header">
        <h2>7. Performance evaluation (speed-up, throughput, weak and strong scaling) and discussion about overheads and optimizations done</h2>
      </div>
      <p> As mentioned above, we ran our parallel model on an AWS EC2 g4dn.12xlarge instance. This model had 4 Nvidia T4 Tensor Core GPUs, which allowed us to test and exploit parallelism in our code. As a baseline, we first ran our code on a single node and increased the number of processes on that node. We plot the amount of time needed for different amount of steps/epochs during our training of the model. We present our plots for this on Zontique and Juvenile below: </p>

        <div class="row">
            <div class="col-md-6">
              <img src="./img/zothique_1node.png" alt="zothique_1node" height="400px">
            </div>
            <div class="col-md-6">
                <img src="./img/juvenile_1node.png" alt="juvenile_1node" height="400px">
            </div>
        </div>
        <br/>

      <p> We then proceed to analyze the parallel results of our code. Instead of varying the number of processes on each node, we now vary the number of nodes for our training phase. We present our plots for this on Zontique and Juvenile below: </p>

      <div class="row">
          <div class="col-md-6">
           <img src="./img/zothique_multinode.png" alt="zothique_multinode" height="400px">            
          </div>
          <div class="col-md-6">
            <img src="./img/juvenile_multinode.png" alt="juvenile_mutlinode" height="400px">
          </div>
      </div>
      <br/>

      <p>By both varying the problem size and number of nodes, we have shown the weak and strong scaling for our parallel application. Interestingly, our parallel "multi-node" case showed slowdown compared to the single node case. In some cases, this slowdown can be severe, showing an almost 2x slowdown when comparing the time needed to perform training on the Juvenile and Zotique datasets when 4 nodes/processes are involved. Moreover, we also noticed that increasing the level of parallelism in our application did not result in any speedups even in the 1 node case. This implies that there is significant overheads which we investigated further. Upon further research investigation, we noticed that the communication costs for our application is immense. As supported by this article <a href = "https://timdettmers.com/2014/10/09/deep-learning-data-parallelism/">here</a>, data parallelism does not yield a good result due to the amount of data that needs to be passed around the nodes. During each backward pass of our training phase, we need to broadcast all gradient values to all other GPUs. With <b> 117 million parameters</b> to train in our smallest example of GPT-2, this represents a huge amount of communication overhead which outweights the computational benefit that we got from scaling the number of processors. 

      Theoretically, there are optimizations that we could have pursued to reduce communication costs in our project. We could have reduced the parameters of the number of parameters with layers such as max pooling or using convolution layers. However, that will edit the fundamental structure of GPT-2 that is empirically tested by OpenAI. Moreover with the initial 117 million parameters to train, it is not feasible to decide on which layers to modify given the timeframe that we have. </p>

      <a class="anchor" id="8"></a>
      <div class="page-header">
        <h2>8. Description of advanced features</h2>
      </div>
      <p> Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod
      tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam,
      quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo
      consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse
      cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non
      proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>

      <a class="anchor" id="9"></a>
      <div class="page-header">
        <h2>9. Final discussion about goals achieved, improvements suggested, lessons learnt, future work, interesting insights</h2>
      </div>
      <p> using the larger gpt-2 models, more work on parallelizing gpt-2, modify the gpt-2 algorithm to be more suited for this task, frame the textual style transfer more rigorously and use the metrics we originally proposed.</p>

      <a class="anchor" id="10"></a>
      <div class="page-header">
        <h2>10. Citations</h2>
      </div>
      <p> https://github.com/orange-erotic-bible credit for the text generation idea (warning, the repo is potentially offensive.)
      https://timdettmers.com/2014/10/09/deep-learning-data-parallelism/</p>

    </div> <!-- /container -->


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha384-nvAa0+6Qg9clwYCGGPpDQLVpLNn0fRaROjHqs13t4Ggj3Ez50XnGQqc/r8MhnRDZ" crossorigin="anonymous"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="../../dist/js/bootstrap.min.js"></script>
    <script src="../../assets/js/docs.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>
