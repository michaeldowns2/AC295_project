<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" href="../../favicon.ico">

    <title>CS205 Final Project</title>

    <!-- NOTE: USING BOOTSTRAP 3.4 -->
    
    <!-- Bootstrap core CSS -->
    <link href="./css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="./css/ie10-viewport-bug-workaround.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="./css/theme.css" rel="stylesheet">

    <!-- IACS stylesheet -->
    <link rel="stylesheet" href="./style/css/iacs.css" />


    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>
    <!-- Fixed navbar -->
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#top">CS205 Project</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="#top">Home</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container theme-showcase" role="main">

      <!-- Main jumbotron for a primary marketing message or call to action -->
      <div class="jumbotron">
        <h1>Textual Style Transfer with GPT-2</h1>
        <p><b>Group 8</b> - Michael <u>Downs</u>, Cameron <u>Hickert</u>, Wen Rui <u>Liau</u>, Wisoo <u>Song</u></p>
      </div>

      <div class="page-header">
        <h1>Overview</h1>
      </div>
        <ol class="list-group">
        <a href="#1" class="list-group-item list-group-item-info">Description of problem and the need for HPC and/or Big Data</a>
        <a href="#2" class="list-group-item list-group-item-success">Description of solution and comparison with existing work on the problem</a>
        <a href="#3" class="list-group-item list-group-item-info">Description of your model and/or data in detail</a>
        <a href="#4" class="list-group-item list-group-item-success">Technical description of the parallel application, programming models, platform and infrastructure</a>
        <a href="#5" class="list-group-item list-group-item-info">Links to repository with source code, evaluation data sets and test cases</a>
        <a href="#6" class="list-group-item list-group-item-success">Technical description of the software design, code baseline, dependencies, how to use the code, and system and environment needed to reproduce your tests</a>
        <a href="#7" class="list-group-item list-group-item-info">Performance evaluation (speed-up, throughput, weak and strong scaling) and discussion about overheads and optimizations done</a>
        <a href="#8" class="list-group-item list-group-item-success">Description of advanced features</a>
        <a href="#9" class="list-group-item list-group-item-info">Final discussion about goals achieved, improvements suggested, lessons learnt, future work, interesting insights</a>
        <a href="#10" class="list-group-item list-group-item-success">Citations</a>
      </ol>

      <a class="anchor" id="1"></a>
      <div class="page-header">
        <h2>1. Description of problem and the need for HPC and/or Big Data</h2>
      </div>
      <p> In our project, we will be exploring the problem of Textual Style Transfer with GPT-2. Two facets of any written work are style and content. <em>Content</em> pertains to the events or ideas being described whilst <em>Style</em> pertains to the manner in which the events or ideas are being described. Different authors and/or mediums have different styles. Textual style transfer aims to modify the style of one work to mimic the style of another while keeping the content relevant to the original. We will be using the GPT-2 model developed by OpenAI to perform textual style transfer on diverse and large datasets. </p>

      <p> There is a need for both big compute and big data processing in our project. On the <b>Big Compute</b> front, we will need a lot of compute resources to train the GPT-2 model for textural style transfer. GPT-2 comes in various sizes and all have a significant number of parameters to train. The "small" model that we are working with has 117 million parameters to train and our rough time estimate shows that we require 6 hours on single GPU to get reasonable (conditional) initial results using 7MB of text and 33k iterations. HPC will allow us to speed this up. </p>

      <p>On the <b>Big Data</b> front, the original GPT-2 model is originally trained with 40GB of internet text. This includes WikiText, which contains 100 million tokens from  verified “good” and “featured” Wikipedia articles as well as Treebank-2 which contains 1 million tokens from WSJ articles. The large sizes of the datasets that we are dealing with implies a clear for Big Data processing for us to handle these cases.</p> 

      <a class="anchor" id="2"></a>
      <div class="page-header">
        <h2>2. Description of solution and comparison with existing work on the problem</h2>
      </div>
      <p> In scoping the existing work on this problem, we refer to the paper and associated codebase written by OpenAI linked <a href = https://openai.com/blog/better-language-models/>here</a>. OpenAI provides the GPT-2 model architecture that they tested for Textual Style Transfer. However, they do not provide the trained parameters for this problem. As such, our solution involves training this model to do textual style transfer. Moreover in comparison with the original solution, our solution will also train the parameters in parallel which we will further elaborate below. </p>

      <a class="anchor" id="3"></a>
      <div class="page-header">
        <h2>3. Description of your model and/or data in detail</h2>
      </div>
      <p> I will first describe the source data that we used for our textual style transfer application. For our source textual data, we used the zothique stories obtained from <a href = khttp://www.eldritchdark.com/> this link </a> and project gutenberg books obtained using from this <a href = https://github.com/pgcorpus/gutenberg/tree/master/src> repository </a>. The raw project gutenberg corpus is almost 9GB compressed that requires processing. We used the super_cleaner functionality from this <a href = https://github.com/kiasar/gutenberg_cleaner> repository </a> to strip of all of the project gutenberg boilerplate. The pgcorpus repository also generates a metadata file while we used to separate the project gutenberg books into the genres: juvenile, 19th century, science fiction, adventure, fairy tales, and poetry. Of those, we only used the first three.</p>

      <p> The core model we used for textual style transfer is GPT-2. GPT-2 is a transformer-based Neural Net for unsupervised Multitask Learning. This model consists of an Encoder as well as a Decoder and the model architecture is seen below. In the original paper by OpenAI, this model is able to perform zero-shot reading comprehension, summarization, translation and question answering to varying degrees of success.</p>

      <img src="./img/gpt2.png" class="center-block" width=400px>


      <p> In describing our model for textual style transfer, I will first outline the core algorithm in generating new text from our corpus of data outlined above. The algorithm for generating text takes as input a pre-trained GPT-2 and a text file. The text file is chunked into contiguous groups of sentences which are used as prompts for GPT-2. The contiguous group is then shifted by some step size. For this project, the chunk/window size was <b>9</b> and the step size was <b>4</b>. 4 sentences were generated at a time using GPT-2. We used nltk as the sentence tokenizer. </p>

      <a class="anchor" id="4"></a>
      <div class="page-header">
        <h2>4. Technical description of the parallel application, programming models, platform and infrastructure</h2>
      </div>
      <p> Our parallel application involves exploiting a range of different parallelism levels including many-node and many-core parallelism as we used multiple GPUs on multiple nodes. There are two main areas to parallelize: the training phase and the generation phase. </p>
      <p> In the training phase of our project, the transformer computations and gradient updates in GPT-2 fine-tuning will be parallelized. In the generation phase, we will be parallelizing the generation of style-transfered text from a source file. </p>
      <p> In deciding on the parallel programming model to use, we used the ring all-reduce algorithm via Horovod that utilizes open MPI. Compared to the alternative of distributed tensorflow's worker-parameter server model, Horovod's ring all-reduce algorithm is superior in terms of both speed as well as space due to the absence of the need for a parameter server. An overview of this parallel algorithm is shown below. </p>

      <img src="./img/ringallreduce.png" class="center-block" width=1000px>

      <p> Our language of choice will be Python which allows us to access a wide range of different libraries for our project. Our machine learning framework used is Tensorflow and Keras as well as Horovod as mentioned above. Our platform operating system used is Deep Learning Amazon Machine Image (Ubuntu) which came with Tensorflow and Horovod pre-configured. Our infrastructure of choice is Amazon's Elastic Compute 2 as well as Harvard's Cannon supercomputer.</p>

      <a class="anchor" id="5"></a>
      <div class="page-header">
        <h2>5. Links to repository with source code, evaluation data sets and test cases</h2>
      </div>
      <p> Our entire codebase for this project is hosted on Wisoo's github page linked <a href=https://github.com/osiajod/cs205_project>here</a>. In that page, we have also linked the a folder titled <u>results</u> which contain our outputs from training our GPT-2 model on a variety of different nodes on AWS. These outputs allow us to better understand the accuracy and speed of scaling of GPT-2 which we will present in the plots below. We have also included several style-transfered texts that our generated from our trained GPT-2 model. These texts are how we qualitatively evaluated the effectiveness of our model in performing the textual style transfer. </p>

      <a class="anchor" id="6"></a>
      <div class="page-header">
        <h2>6. Technical description of the software design, code baseline, dependencies, how to use the code, and system and environment needed to reproduce your tests</h2>
      </div>
      <p> <b>Will probably need to fill in our instructions on running.</b></p>

      <a class="anchor" id="7"></a>
      <div class="page-header">
        <h2>7. Performance evaluation (speed-up, throughput, weak and strong scaling) and discussion about overheads and optimizations done</h2>
      </div>
      <p> Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod
      tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam,
      quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo
      consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse
      cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non
      proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>

      <a class="anchor" id="8"></a>
      <div class="page-header">
        <h2>8. Description of advanced features</h2>
      </div>
      <p> Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod
      tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam,
      quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo
      consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse
      cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non
      proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>

      <a class="anchor" id="9"></a>
      <div class="page-header">
        <h2>9. Final discussion about goals achieved, improvements suggested, lessons learnt, future work, interesting insights</h2>
      </div>
      <p> using the larger gpt-2 models, more work on parallelizing gpt-2, modify the gpt-2 algorithm to be more suited for this task, frame the textual style transfer more rigorously and use the metrics we originally proposed.</p>

      <a class="anchor" id="10"></a>
      <div class="page-header">
        <h2>10. Citations</h2>
      </div>
      <p> https://github.com/orange-erotic-bible credit for the text generation idea (warning, the repo is potentially offensive.)</p>

    </div> <!-- /container -->


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha384-nvAa0+6Qg9clwYCGGPpDQLVpLNn0fRaROjHqs13t4Ggj3Ez50XnGQqc/r8MhnRDZ" crossorigin="anonymous"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="../../dist/js/bootstrap.min.js"></script>
    <script src="../../assets/js/docs.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>
